{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# konlpy 관련 참고링크\n",
    "# https://velog.io/@soo-im/konlpy-%EC%84%A4%EC%B9%98-%EC%97%90%EB%9F%AC-%ED%95%B4%EA%B2%B0%EC%B1%85-%EC%95%84%EB%82%98%EC%BD%98%EB%8B%A4-JPYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\colacan\\documents\\github\\league_of_legend_challenges_project\\jpype1-1.1.2-cp38-cp38-win_amd64.whl\n",
      "JPype1 is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n"
     ]
    }
   ],
   "source": [
    "!pip install JPype1-1.1.2-cp38-cp38-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from sentence_transformers) (0.1.91)\n",
      "Requirement already satisfied: torchvision in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from sentence_transformers) (0.12.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from sentence_transformers) (4.64.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from sentence_transformers) (1.1.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from sentence_transformers) (3.7)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from sentence_transformers) (1.11.0+cu113)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from sentence_transformers) (0.0.12)\n",
      "Requirement already satisfied: numpy in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from sentence_transformers) (1.22.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from sentence_transformers) (1.8.1)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Using cached transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (4.2.0)\n",
      "Collecting huggingface-hub\n",
      "  Using cached huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Using cached tokenizers-0.12.1-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: requests in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.18.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.4.24)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from nltk->sentence_transformers) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from nltk->sentence_transformers) (8.1.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from torchvision->sentence_transformers) (9.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.22)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.5.18)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.8.1rc1\n",
      "    Uninstalling tokenizers-0.8.1rc1:\n",
      "      Successfully uninstalled tokenizers-0.8.1rc1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] 액세스가 거부되었습니다: 'C:\\\\Users\\\\colacan\\\\anaconda3\\\\envs\\\\leagueoflegend\\\\Lib\\\\site-packages\\\\~-kenizers\\\\tokenizers.cp38-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from konlpy) (4.8.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from konlpy) (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from konlpy) (1.22.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers\n",
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from pandas) (1.22.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(name):\n",
    "    df = pd.read_csv(name)\n",
    "    df = df.drop(columns='Unnamed: 0')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_keywords(df):\n",
    "\n",
    "    doc = ''\n",
    "    for text_list in df['full_text']:\n",
    "        doc += (' ' + text_list)\n",
    "\n",
    "    # 형태소 분석기\n",
    "    okt = Okt()\n",
    "\n",
    "    tokenized_doc = okt.pos(doc)\n",
    "    tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'Noun'])\n",
    "\n",
    "    print('품사 태깅 10개만 출력 :',tokenized_doc[:10])\n",
    "    print('명사 10개만 출력 :',tokenized_nouns[:10])\n",
    "\n",
    "    # 사이킷런의 CountVectorizer를 사용하여 단어를 추출\n",
    "    n_gram_range = (2, 3)\n",
    "\n",
    "    count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])\n",
    "    candidates = count.get_feature_names_out()\n",
    "\n",
    "    print('trigram 개수 :',len(candidates))\n",
    "    print('trigram 다섯개만 출력 :',candidates[:5])\n",
    "\n",
    "    # 다국어 SBERT 로드\n",
    "    model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\n",
    "    doc_embedding = model.encode([doc])\n",
    "    candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "    # 문서와 가장 유사한 키워드 추출\n",
    "    top_n = 100\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "    keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "    print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_league = clean_df('C:\\\\Users\\\\colacan\\\\Documents\\\\GitHub\\\\League_Of_Legend_Challenges_Project\\\\tweet_list\\\\tweet_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "품사 태깅 10개만 출력 : [('@ezpz99wp', 'ScreenName'), ('리그', 'Noun'), ('오브', 'Noun'), ('레전드', 'Noun'), ('의', 'Josa'), ('조이', 'Noun'), ('이신가', 'Josa'), ('보군요', 'Verb'), ('상대', 'Noun'), ('를', 'Josa')]\n",
      "명사 10개만 출력 : 리그 오브 레전드 \n",
      "trigram 개수 : 1940\n",
      "trigram 다섯개만 출력 : ['가격 상관' '가격 상관 대여' '가능 가능' '가능 가능 자랭' '가능 아주']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\colacan/.cache\\torch\\sentence_transformers\\sentence-transformers_xlm-r-100langs-bert-base-nli-stsb-mean-tokens\\ were not used when initializing XLMRobertaModel: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "special token mask_token has to be either str or AddedToken but got: <class 'dict'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\colacan\\Documents\\GitHub\\League_Of_Legend_Challenges_Project\\keybert.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/colacan/Documents/GitHub/League_Of_Legend_Challenges_Project/keybert.ipynb#ch0000009?line=0'>1</a>\u001b[0m most_keywords(df_league)\n",
      "\u001b[1;32mc:\\Users\\colacan\\Documents\\GitHub\\League_Of_Legend_Challenges_Project\\keybert.ipynb Cell 8'\u001b[0m in \u001b[0;36mmost_keywords\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/colacan/Documents/GitHub/League_Of_Legend_Challenges_Project/keybert.ipynb#ch0000007?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtrigram 다섯개만 출력 :\u001b[39m\u001b[39m'\u001b[39m,candidates[:\u001b[39m5\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/colacan/Documents/GitHub/League_Of_Legend_Challenges_Project/keybert.ipynb#ch0000007?line=24'>25</a>\u001b[0m \u001b[39m# 다국어 SBERT 로드\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/colacan/Documents/GitHub/League_Of_Legend_Challenges_Project/keybert.ipynb#ch0000007?line=25'>26</a>\u001b[0m model \u001b[39m=\u001b[39m SentenceTransformer(\u001b[39m'\u001b[39;49m\u001b[39msentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/colacan/Documents/GitHub/League_Of_Legend_Challenges_Project/keybert.ipynb#ch0000007?line=26'>27</a>\u001b[0m doc_embedding \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode([doc])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/colacan/Documents/GitHub/League_Of_Legend_Challenges_Project/keybert.ipynb#ch0000007?line=27'>28</a>\u001b[0m candidate_embeddings \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode(candidates)\n",
      "File \u001b[1;32mc:\\Users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:94\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/sentence_transformers/SentenceTransformer.py?line=85'>86</a>\u001b[0m     snapshot_download(model_name_or_path,\n\u001b[0;32m     <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/sentence_transformers/SentenceTransformer.py?line=86'>87</a>\u001b[0m                         cache_dir\u001b[39m=\u001b[39mcache_folder,\n\u001b[0;32m     <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/sentence_transformers/SentenceTransformer.py?line=87'>88</a>\u001b[0m                         library_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msentence-transformers\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/sentence_transformers/SentenceTransformer.py?line=88'>89</a>\u001b[0m                         library_version\u001b[39m=\u001b[39m__version__,\n\u001b[0;32m     <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/sentence_transformers/SentenceTransformer.py?line=89'>90</a>\u001b[0m                         ignore_files\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mflax_model.msgpack\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrust_model.ot\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtf_model.h5\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/sentence_transformers/SentenceTransformer.py?line=90'>91</a>\u001b[0m                         use_auth_token\u001b[39m=\u001b[39muse_auth_token)\n\u001b[0;32m     <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/sentence_transformers/SentenceTransformer.py?line=92'>93</a>\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(model_path, \u001b[39m'\u001b[39m\u001b[39mmodules.json\u001b[39m\u001b[39m'\u001b[39m)):    \u001b[39m#Load as SentenceTransformer model\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/sentence_transformers/SentenceTransformer.py?line=93'>94</a>\u001b[0m     modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_sbert_model(model_path)\n\u001b[0;32m     <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/sentence_transformers/SentenceTransformer.py?line=94'>95</a>\u001b[0m \u001b[39melse\u001b[39;00m:   \u001b[39m#Load with AutoModel\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/sentence_transformers/SentenceTransformer.py?line=95'>96</a>\u001b[0m     modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_auto_model(model_path)\n",
      "File \u001b[1;32mc:\\Users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:831\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[1;34m(self, model_path)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/sentence_transformers/SentenceTransformer.py?line=828'>829</a>\u001b[0m \u001b[39mfor\u001b[39;00m module_config \u001b[39min\u001b[39;00m modules_config:\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/sentence_transformers/SentenceTransformer.py?line=829'>830</a>\u001b[0m     module_class \u001b[39m=\u001b[39m import_from_string(module_config[\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/sentence_transformers/SentenceTransformer.py?line=830'>831</a>\u001b[0m     module \u001b[39m=\u001b[39m module_class\u001b[39m.\u001b[39;49mload(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(model_path, module_config[\u001b[39m'\u001b[39;49m\u001b[39mpath\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/sentence_transformers/SentenceTransformer.py?line=831'>832</a>\u001b[0m     modules[module_config[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m module\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/sentence_transformers/SentenceTransformer.py?line=833'>834</a>\u001b[0m \u001b[39mreturn\u001b[39;00m modules\n",
      "File \u001b[1;32mc:\\Users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:137\u001b[0m, in \u001b[0;36mTransformer.load\u001b[1;34m(input_path)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/sentence_transformers/models/Transformer.py?line=134'>135</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(sbert_config_path) \u001b[39mas\u001b[39;00m fIn:\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/sentence_transformers/models/Transformer.py?line=135'>136</a>\u001b[0m     config \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(fIn)\n\u001b[1;32m--> <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/sentence_transformers/models/Transformer.py?line=136'>137</a>\u001b[0m \u001b[39mreturn\u001b[39;00m Transformer(model_name_or_path\u001b[39m=\u001b[39;49minput_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig)\n",
      "File \u001b[1;32mc:\\Users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:31\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[1;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/sentence_transformers/models/Transformer.py?line=27'>28</a>\u001b[0m config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_args, cache_dir\u001b[39m=\u001b[39mcache_dir)\n\u001b[0;32m     <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/sentence_transformers/models/Transformer.py?line=28'>29</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_model(model_name_or_path, config, cache_dir)\n\u001b[1;32m---> <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/sentence_transformers/models/Transformer.py?line=30'>31</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(tokenizer_name_or_path \u001b[39mif\u001b[39;49;00m tokenizer_name_or_path \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m model_name_or_path, cache_dir\u001b[39m=\u001b[39;49mcache_dir, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtokenizer_args)\n\u001b[0;32m     <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/sentence_transformers/models/Transformer.py?line=32'>33</a>\u001b[0m \u001b[39m#No max_seq_length set. Try to infer from model\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/sentence_transformers/models/Transformer.py?line=33'>34</a>\u001b[0m \u001b[39mif\u001b[39;00m max_seq_length \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages\\transformers\\tokenization_auto.py:217\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_auto.py?line=214'>215</a>\u001b[0m             \u001b[39mreturn\u001b[39;00m tokenizer_class_fast\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_auto.py?line=215'>216</a>\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_auto.py?line=216'>217</a>\u001b[0m             \u001b[39mreturn\u001b[39;00m tokenizer_class_py\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_auto.py?line=218'>219</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_auto.py?line=219'>220</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to build an AutoTokenizer.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_auto.py?line=220'>221</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_auto.py?line=221'>222</a>\u001b[0m         config\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m TOKENIZER_MAPPING\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_auto.py?line=222'>223</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_auto.py?line=223'>224</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1140\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils_base.py?line=1086'>1087</a>\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils_base.py?line=1087'>1088</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils_base.py?line=1088'>1089</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils_base.py?line=1089'>1090</a>\u001b[0m \u001b[39m    Instantiate a :class:`~transformers.PreTrainedTokenizer` (or a derived class) from a predefined tokenizer.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils_base.py?line=1090'>1091</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils_base.py?line=1137'>1138</a>\u001b[0m \n\u001b[0;32m   <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils_base.py?line=1138'>1139</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils_base.py?line=1139'>1140</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1287\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils_base.py?line=1284'>1285</a>\u001b[0m \u001b[39m# Instantiate tokenizer.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils_base.py?line=1285'>1286</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils_base.py?line=1286'>1287</a>\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils_base.py?line=1287'>1288</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils_base.py?line=1288'>1289</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m   <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils_base.py?line=1289'>1290</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils_base.py?line=1290'>1291</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils_base.py?line=1291'>1292</a>\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages\\transformers\\tokenization_xlm_roberta.py:120\u001b[0m, in \u001b[0;36mXLMRobertaTokenizer.__init__\u001b[1;34m(self, vocab_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_xlm_roberta.py?line=107'>108</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_xlm_roberta.py?line=108'>109</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_xlm_roberta.py?line=109'>110</a>\u001b[0m     vocab_file,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_xlm_roberta.py?line=117'>118</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_xlm_roberta.py?line=118'>119</a>\u001b[0m ):\n\u001b[1;32m--> <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_xlm_roberta.py?line=119'>120</a>\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_xlm_roberta.py?line=120'>121</a>\u001b[0m         bos_token\u001b[39m=\u001b[39;49mbos_token,\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_xlm_roberta.py?line=121'>122</a>\u001b[0m         eos_token\u001b[39m=\u001b[39;49meos_token,\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_xlm_roberta.py?line=122'>123</a>\u001b[0m         unk_token\u001b[39m=\u001b[39;49munk_token,\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_xlm_roberta.py?line=123'>124</a>\u001b[0m         sep_token\u001b[39m=\u001b[39;49msep_token,\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_xlm_roberta.py?line=124'>125</a>\u001b[0m         cls_token\u001b[39m=\u001b[39;49mcls_token,\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_xlm_roberta.py?line=125'>126</a>\u001b[0m         pad_token\u001b[39m=\u001b[39;49mpad_token,\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_xlm_roberta.py?line=126'>127</a>\u001b[0m         mask_token\u001b[39m=\u001b[39;49mmask_token,\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_xlm_roberta.py?line=127'>128</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_xlm_roberta.py?line=128'>129</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_xlm_roberta.py?line=130'>131</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_xlm_roberta.py?line=131'>132</a>\u001b[0m         \u001b[39mimport\u001b[39;00m \u001b[39msentencepiece\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mspm\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages\\transformers\\tokenization_utils.py:157\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils.py?line=155'>156</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils.py?line=156'>157</a>\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils.py?line=158'>159</a>\u001b[0m     \u001b[39m# Added tokens - We store this for both slow and fast tokenizers\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils.py?line=159'>160</a>\u001b[0m     \u001b[39m# until the serialization of Fast tokenizers is updated\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils.py?line=160'>161</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madded_tokens_encoder: Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1046\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils_base.py?line=1039'>1040</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_side \u001b[39min\u001b[39;00m [\n\u001b[0;32m   <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils_base.py?line=1040'>1041</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mright\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils_base.py?line=1041'>1042</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils_base.py?line=1042'>1043</a>\u001b[0m ], \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPadding side should be selected between \u001b[39m\u001b[39m'\u001b[39m\u001b[39mright\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, current value: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_side\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils_base.py?line=1043'>1044</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mmodel_input_names\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names)\n\u001b[1;32m-> <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils_base.py?line=1045'>1046</a>\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\colacan\\anaconda3\\envs\\leagueoflegend\\lib\\site-packages\\transformers\\tokenization_utils_base.py:606\u001b[0m, in \u001b[0;36mSpecialTokensMixin.__init__\u001b[1;34m(self, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils_base.py?line=603'>604</a>\u001b[0m     \u001b[39msetattr\u001b[39m(\u001b[39mself\u001b[39m, key, value)\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils_base.py?line=604'>605</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils_base.py?line=605'>606</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils_base.py?line=606'>607</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mspecial token \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m has to be either str or AddedToken but got: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(key, \u001b[39mtype\u001b[39m(value))\n\u001b[0;32m    <a href='file:///c%3A/Users/colacan/anaconda3/envs/leagueoflegend/lib/site-packages/transformers/tokenization_utils_base.py?line=607'>608</a>\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: special token mask_token has to be either str or AddedToken but got: <class 'dict'>"
     ]
    }
   ],
   "source": [
    "most_keywords(df_league)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top = clean_df('top_list.csv')\n",
    "most_keywords(df_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "품사 태깅 10개만 출력 : [('@tflno_dxz23', 'ScreenName'), ('다른사람', 'Noun'), ('들', 'Suffix'), ('이', 'Josa'), ('낀거라도', 'Verb'), ('보면서', 'Verb'), ('힐링', 'Noun'), ('롤', 'Noun'), ('할려', 'Verb'), ('구', 'Noun')]\n",
      "명사 10개만 출력 : 다른사람 힐링 롤 \n",
      "trigram 개수 : 800\n",
      "trigram 다섯개만 출력 : ['가끔 서폿' '가끔 서폿 원딜' '가끔 스왑' '가끔 스왑 바텀' '가능 트위터']\n",
      "['넷이 인베 방어', '미드 시즌', '해명 트윗 리리아', '미드 미드 원딜', '티저 오락 엉덩이', '어푸푸 오늘', '남아 비바 리움', '터트렸는데 상체 쌍련들', '정글 넷이 인베', '인베 방어', '바텀 사람 게임', '카구야 학생회', '꾀죄죄 후즐', '미드 원딜 정글', '서폿 원딜 정글', '미드 바텀', '고정 미드 원딜', '벽력 쭉쭉 라인', '가렌 제이스 피오라', '미드 미식', '미코 차이 지면', '정글 돌기 바텀', '미드 카구야 학생회', '솔킬 미드', '가면 탱서폿 미드', '가미 유우', '자꾸 서폿 혜지', '미드 정글 구해', '포지션 미코', '미드 키사', '웅애 서폿', '제이스 피오라 오공', '꾀죄죄 후즐 남편', '처음 바텀 유미', '미드 라인 예정', '치카 미드 가미', '노틸러스 챔피언 특정', '거시 어푸푸 오늘', '커버 역전승 친구', '치어리딩 제일 캐릭터', '원딜 게임 터트려', '시간 모스트 벨코즈', '하싈분 웅애 서폿', '미드 꾀죄죄 후즐', '낭만 원딜 서폿', '챔피언 특정 챔피언', '린처 모스트 미드', '킹반 린처 모스트', '상대로 솔킬 미드', '오오 토리 미드', '거시 어푸푸', '학생회 포지션 미코', '리그 티저', '카구야 학생회 포지션', '대전 젠츠', '명구 카구야 학생회', '학살 이유 솔킬따', '어푸푸 오늘 미드', '솔킬따 정글 옵젝', '때문 메이지 유저', '오공 말파 켄치', '라인 챔프 게임', '미드 낭만 원딜', '바텀 유미', '챔프 스웨인 미드', '서폿 노틸러스 챔피언', '유우 봇듀', '참여 미드 서폿', '봇듀 회장 원딜', '리턴 거시 어푸푸', '서폿 고정 미드', '매력 타입 치어리딩', '미드 가미 유우', '바텀 남자 플레이', '토리 미드 키사', '정글 이정 티어', '노틸러스 챔피언', '상대 미드 키배', '젠츠 하싈분 웅애', '머싯단 대전', '서폿 미드 유저', '시즌 리그', '미드 미드 시즌', '옵젝 미드 낭만', '부장 매력 타입', '가미 유우 봇듀', '대전 젠츠 미드', '미식 축구 부장', '정공 상대 미드', '인큐 정글 서폿', '웅애 서폿 미드', '캐리 이상 바텀터지', '시즌 리그 티저', '리그 티저 오락', '미드 시즌 리그', '대전 젠츠 하싈분', '젠츠 머싯단 대전', '라인 복귀 리쉬', '바텀 유미 미드', '머싯단 대전 젠츠']\n"
     ]
    }
   ],
   "source": [
    "df_mid = clean_df('mid_list.csv')\n",
    "most_keywords(df_mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "품사 태깅 10개만 출력 : [('와', 'Verb'), (',..', 'Punctuation'), ('씨발', 'Noun'), ('간', 'Suffix'), ('만에', 'Josa'), ('롤', 'Noun'), ('했더니', 'Verb'), ('요즘', 'Noun'), ('현자', 'Noun'), ('처럼', 'Josa')]\n",
      "명사 10개만 출력 : 씨발 롤 요즘 현자\n",
      "trigram 개수 : 853\n",
      "trigram 다섯개만 출력 : ['가기 프로젝트' '가기 프로젝트 구합' '가끔 정답' '가끔 정답 라인' '가미 유우']\n",
      "['상대 납골당 슈가', '챔피언 특정 챔피언', '랭크하닥 멘탈 정글', '픽해 듀오 챔피언', '진짜 개패 정글', '연구원 리듬게임', '대해 얘기 미드', '낭만 원딜', '뭔가 계속 차이', '보통 참여 미드', '간만 쌍욕 오늘', '카구야 서폿 느낌', '라면 만하 자꾸', '보통 상대 정글', '이재석 욕설 트롤', '일반 예전 렙때', '토리 미드 키사', '게임 챔프 스웨인', '예전 렙때 암살자', '기생 코바 원딜', '피지 가끔', '챔피언 이유 때문', '탈론 운영 달라', '미드 정글 구해', '언랭 언랭 여자', '참여 티어 언랭', '라면 만하', '정글 때문 메이지', '리쉬 보통 참여', '간만 쌍욕', '진짜 이해 처음', '정글 지금 채팅', '평생 행복 하자', '내전 참여 백구', '미드 낭만 원딜', '여자 인척 쌍벞주', '챔피언 정글 혼자', '저격 소리 참여', '바텀 유미 미드', '파우더 목표 카구야', '시발 대해 얘기', '라면 원영 몰입', '탱커 하싈분 웅애', '예능 제일', '라면 대학원생 아마', '진짜 진심 골드', '프로젝트 구합 승리', '라인 크립 보통', '꼽주 소리 내전', '덤덤 이빨 거시', '라인 눈앞 상대', '하나 몰루는데', '만하 랭크하닥 멘탈', '렙때 암살자 상대로', '유우 봇듀', '미드 가미 유우', '정글 만하 랭크하닥', '유미 유미 새끼', '문제 고요 게임', '승리 지향 즐겜', '봇듀 회장 원딜', '분만 만하 자꾸', '암살자 상대로 솔킬', '노틸러스 요즘 정글', '어딧음 심지어 가정', '미드 낭만', '지향 즐겜', '사람 예능', '진짜 소름 우리', '몰루는데 블리츠 바람', '하나 몰루는데 블리츠', '즐겜 모드', '샛기 어딧음 심지어', '관련 연구원 리듬게임', '욕설 트롤 그때', '목적 상대 캐치', '문제 재미', '도수 이재석 욕설', '처음 하든', '트롤 그때 챔피언', '캐치 묘수풀이 느낌', '즐겜 모드 조건', '피지 가끔 정답', '라면 혹시 트친', '욕설 트롤', '가미 유우 봇듀', '바람 문제 재미', '연구원 리듬게임 마이', '재미 진짜 소름', '문제 재미 진짜', '지향 즐겜 모드', '상대 캐치 묘수풀이', '느낌 피지 가끔', '사람 예능 제일', '정글 하나 몰루는데', '이해 처음 하든', '언랭 여자 인척', '예능 제일 스토리', '크립 보통 상대', '묘수풀이 느낌 피지']\n"
     ]
    }
   ],
   "source": [
    "df_jungle = clean_df('jungle_list.csv')\n",
    "most_keywords(df_jungle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "품사 태깅 10개만 출력 : [('@bebeahrang', 'ScreenName'), ('발', 'Noun'), ('로란', 'Noun'), ('트', 'Noun'), ('같', 'Adjective'), ('이해', 'Noun'), ('요', 'Josa'), ('엄청', 'Adverb'), ('재미있는', 'Adjective'), ('게임', 'Noun')]\n",
      "명사 10개만 출력 : 발 로란 트 이해 \n",
      "trigram 개수 : 920\n",
      "trigram 다섯개만 출력 : ['가끔 미니' '가끔 미니 언도' '가끔 서폿' '가끔 서폿 원딜' '가끔 스왑']\n",
      "['환영 적막 게임', '하싈분 웅애 서폿', '팀운 하타 버러지', '게임 하싈분', '하이 텐션 환영', '딜로 유격대 마딜', '사람 친구 이상', '계약 다해', '가끔 실수', '잘해야 원딜 캐리', '유격대 마딜 그냥', '미드 원딜 정글', '캐리 이상 바텀터지', '원딜 중반 이후', '가끔 미니 언도', '라이너 명구 그냥', '미드 가미 유우', '사리 원딜 아기', '원딜 유격대', '보육 힐러 지오', '원딜 유격대 딜로', '원딜 제껍니', '기생 코바 원딜', '게임 터트려 아무', '아하 효월 원딜', '폿도 잘해야', '바텀터지 원딜 구원불', '비밀 물리 원딜', '유우 봇듀', '가면 서폿 그냥', '미드 가끔 서폿', '원딜 암살자 발견', '챔피언 정글 혼자', '버러지 아하 효월', '화남 연패', '제일 이해 원딜', '진짜 트롤 칼리', '바람 무관 대신', '진짜 박하', '로란 이해 게임', '진짜 진심 골드', '마법 미드 리리아', '유격대 딜로 유격대', '게임 카구야 학생회', '게임 터트렸는데', '터트렸는데 상체', '리쉬 보통 참여', '레드 가끔 실수', '화남 연패 동생', '마딜 그냥 비술사', '터트려 아무 원딜', '이해 혼자 친구', '주로 미드 가끔', '먼저 원딜 무도', '마법 미드 아하', '봇듀 회장 원딜', '원딜 주로 라인', '어쩌 어쩌 막상', '원딜 계약 다해', '제껍니 마이크 필수', '설치 사람 일단', '원딜 제껍니 마이크', '존나 화남 연패', '닉값 눈치 원딜', '혼자 친구 바람', '학살 이유 솔킬따', '진짜 트롤', '전사 먼저 원딜', '해괴 논리 진짜', '서폿 이해 혼자', '근딜거 평생', '가끔 서폿 원딜', '복귀 진짜 트롤', '계약 다해 진짜', '폿도 잘해야 원딜', '이상 엇으면어캄 게임', '그냥 비술사 보시', '가미 유우 봇듀', '원딜 게임 터트려', '친구 바람 혼자', '무관 대신 참고', '사람 게임 터트렸는데', '평생 웃기', '다해 진짜 박하', '친구 이상 엇으면어캄', '가끔 실수 막타침', '평생 웃기 그거슨', '게임 하싈분 웅애', '논리 진짜 트롤', '근딜 근딜거 평생', '정글 원딜 버러지', '엇으면어캄 게임 그냥', '막타침 이건 스킬', '게임 터트렸는데 상체', '웃기 그거슨 편견', '연습 원딜 제껍니', '게임 폿도 잘해야', '엇으면어캄 게임 카구야', '터트렸는데 상체 쌍련들', '근딜거 평생 웃기']\n"
     ]
    }
   ],
   "source": [
    "df_ad = clean_df('ad_list.csv')\n",
    "most_keywords(df_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "품사 태깅 10개만 출력 : [('@bebeahrang', 'ScreenName'), ('발', 'Noun'), ('로란', 'Noun'), ('트', 'Noun'), ('같', 'Adjective'), ('이해', 'Noun'), ('요', 'Josa'), ('엄청', 'Adverb'), ('재미있는', 'Adjective'), ('게임', 'Noun')]\n",
      "명사 10개만 출력 : 발 로란 트 이해 \n",
      "trigram 개수 : 890\n",
      "trigram 다섯개만 출력 : ['가끔 미니' '가끔 미니 언도' '가끔 서폿' '가끔 서폿 원딜' '가끔 스왑']\n",
      "['사람 요즘 서폿', '모름 진짜 흡수', '진짜 빡쵸 실속', '관둔적잇엇음 진짜 지인', '대신 분노 주의', '사람 관둔적잇엇음 카구야', '캐리 이상 바텀터지', '서폿 전부 게임', '체감 공감 해도', '이태정 피방 상상', '크게 유미 유미', '주로 탱커 문도', '가끔 실수', '유미 유미 새끼', '진짜 트롤 칼리', '게임 사람 시작', '트윗 양해 바람', '사과 트윗', '주로 미드 가끔', '봇듀 회장 원딜', '마법 마법 미드', '개판 게임', '운영 타도 언제', '그냥 바보 눈물', '가끔 욱하', '트롤 서폿', '가끔 미니 언도', '어쩌 어쩌 막상', '전부 게임 사람', '미드 가미 유우', '뉴비 트윗', '벌레 시작 진짜', '리턴 거시 어푸푸', '초반 크게 유미', '점철 보이 채팅', '탱서폿 미드 마법', '지인 우리 혜지', '티어 서은광 혹시', '무관 대신 참고', '리쉬 보통 참여', '폿도 잘해야 원딜', '탱커 문도 갑자기', '설치 사람 일단', '가끔 욱하 아버', '가끔 서폿 원딜', '바람 트롤', '계약 다해 진짜', '적용 그냥 바보', '싹싹 그때 벌레', '다해 진짜 박하', '예정 오빠 픈데', '마법 미드 자식', '마법 미드 갑자기', '어푸푸 오늘 미드', '그때 벌레 시작', '특정 챔피언 정글', '이빨 거시 정글', '얼마 안대 서폿챔', '편지 진짜 빡쵸', '학생회 포지션 미코', '게임 해도', '사과 트윗 등등', '개판 게임 해도', '해도 온갖 혐오', '진짜 개판 게임', '눈물 가끔 욱하', '덤덤 이빨 거시', '어쩌 막상 게임', '유우 봇듀', '코딩 사천성', '트윗 양해', '최대한 딜넣음 오늘', '마법 미드 강우', '진짜 트롤', '트롤 칼리', '픈데 티어 서은광', '갑자기 쭈구러듬', '로란 이해 게임', '서폿 코딩 사천성', '갑자기 쭈구러듬 협곡', '미드 진짜 트롤', '이어도 얘기 혐오', '시작 이어도 얘기', '게임 는걸 원딜', '뉴비 트윗 양해', '말파 최대한 딜넣음', '양해 바람 트롤', '코딩 사천성 주변', '사람 시작 이어도', '막타침 이건 스킬', '가미 유우 봇듀', '제일 뉴비 트윗', '트롤 서폿 깜빡', '바보 눈물 가끔', '게임 해도 온갖', '게임 폿도 잘해야', '가끔 실수 막타침', '트롤 칼리 스타', '문도 갑자기 쭈구러듬', '유미 진짜 트롤']\n"
     ]
    }
   ],
   "source": [
    "df_sup = clean_df('sup_list.csv')\n",
    "most_keywords(df_sup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1d1ca04bcf6d8a22391b842d1bb739acd183735045253f19b5d1acca0a11b0e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('leagueoflegend')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
