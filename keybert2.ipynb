{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "60gZ3iid4Eb3"
      },
      "outputs": [],
      "source": [
        "# konlpy 관련 참고링크\n",
        "# https://velog.io/@soo-im/konlpy-%EC%84%A4%EC%B9%98-%EC%97%90%EB%9F%AC-%ED%95%B4%EA%B2%B0%EC%B1%85-%EC%95%84%EB%82%98%EC%BD%98%EB%8B%A4-JPYPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_lkSl3N64Eb6",
        "outputId": "cde1ea36-cbff-46d1-f26f-7b43a31769bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Requirement 'JPype1-1.1.2-cp38-cp38-win_amd64.whl' looks like a filename, but the file does not exist\u001b[0m\n",
            "\u001b[31mERROR: JPype1-1.1.2-cp38-cp38-win_amd64.whl is not a supported wheel on this platform.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install JPype1-1.1.2-cp38-cp38-win_amd64.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QFvUd71d4Eb7",
        "outputId": "2d8d3e34-4cd2-42a3-ea6e-a514d42a9099",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.7/dist-packages (2.2.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.6.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.2.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.11.0+cu113)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.19.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.12.0+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.64.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.1.96)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence_transformers) (4.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (4.11.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers\n",
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "l2NFPSTJ4Eb7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8B92Teek4Eb8",
        "outputId": "2c0dc923-7828-4258-bc09-93dca426c9da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "C8GyulUm4Eb8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hi9BMEP14Eb8"
      },
      "outputs": [],
      "source": [
        "def clean_df(name):\n",
        "    df = pd.read_csv(name)\n",
        "    df = df.drop(columns='Unnamed: 0')\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jvp-wPUC4Eb9"
      },
      "outputs": [],
      "source": [
        "def most_keywords(df):\n",
        "\n",
        "    doc = ''\n",
        "    for text_list in df['full_text']:\n",
        "        doc += (' ' + text_list)\n",
        "\n",
        "    # 형태소 분석기\n",
        "    okt = Okt()\n",
        "\n",
        "    tokenized_doc = okt.pos(doc)\n",
        "    tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'Noun'])\n",
        "\n",
        "    print('품사 태깅 10개만 출력 :',tokenized_doc[:10])\n",
        "    print('명사 10개만 출력 :',tokenized_nouns[:10])\n",
        "\n",
        "    # 사이킷런의 CountVectorizer를 사용하여 단어를 추출\n",
        "    n_gram_range = (1, 2)\n",
        "\n",
        "    count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])\n",
        "    candidates = count.get_feature_names_out()\n",
        "\n",
        "    print('trigram 개수 :',len(candidates))\n",
        "    print('trigram 다섯개만 출력 :',candidates[:5])\n",
        "\n",
        "    # 다국어 SBERT 로드\n",
        "    model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\n",
        "    doc_embedding = model.encode([doc])\n",
        "    candidate_embeddings = model.encode(candidates)\n",
        "\n",
        "    # 문서와 가장 유사한 키워드 추출\n",
        "    top_n = 100\n",
        "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
        "    keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
        "    print(keywords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "I7hV9Zsz4Eb-"
      },
      "outputs": [],
      "source": [
        "df_league = clean_df('tweet_list.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fSl8H2574Eb-",
        "outputId": "140f2f4a-365d-4cab-eb68-c4cdf138ca9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "품사 태깅 10개만 출력 : [('@ezpz99wp', 'ScreenName'), ('리그', 'Noun'), ('오브', 'Noun'), ('레전드', 'Noun'), ('의', 'Josa'), ('조이', 'Noun'), ('이신가', 'Josa'), ('보군요', 'Verb'), ('상대', 'Noun'), ('를', 'Josa')]\n",
            "명사 10개만 출력 : 리그 오브 레전드 \n",
            "trigram 개수 : 1442\n",
            "trigram 다섯개만 출력 : ['가격' '가격 상관' '가능' '가능 가능' '가능 아주']\n",
            "['직장인', '체스', '토너먼트 레전드', '인기 교류', '시기 무리', '적수 공포', '광기 살육', '틈새시장', '레전드 욕구', '업무', '워치 리듬', '대회 생방송', '게이머', '워크래프트 철권', '시뮬레이션', '경기 재훈', '게임 레인보우', '스포츠선수', '시즌 경험', '스포츠', '컨트롤 인기', '이스포츠', '컴퓨터', '오늘 게임', '메인 게임', '공포 축구', '스포츠 페스티벌', '경기', '최대 경기', '체스 리그', '레전드 스포츠선수', '재미 사회생활', '스포츠 종목', '온라인 자택', '리듬 게임', '해설 컴퓨터', '성인 남성', '이스포츠 영향', '다른 게임', '경기 시간', '틈새시장 나름', '프로 경기', '인수 라이엇게임즈', '선수단 운영', '무리 남자', '비디오 게임', '레전드 경기', '경기 스포츠', '롤토 체스', '전반 이스포츠', '진행 게임', '로란 워크래프트', '게임 리그', '이스포츠 종주국', '하필 스포츠', '전략 게임', '라이엇게임즈', '게임 개발', '게임 배틀', '합성어 게임', '매니아 입문', '게임 주말', '스포츠선수 소리', '워크래프트', '게임 가능', '온라인 배틀', '출현 게임', '헛소리 게임', '라이엇 게임', '시뮬레이션 게임', '지식 플레이', '국산 게임', '레전드 게임', '게임', '프로게임단 협업', '프로게임단 프로게임단', '서브 게임', '프로게임단', '성공 게이머', '게임 사가', '게임 형식', '생각 이스포츠', '게임 에이펙', '게임 서코', '게임 월드', '인기 게임', '게임 사이', '멀티플레이어 온라인', '게임 해설', '게임 대회', '게임 위주', '여러가지 게임', '게임 시간대', '게임 명의', '운영 프로게임단', '요즘 경기', '남성 게임', '망겜 게임', '게임 직장인', '직장인 게임']\n"
          ]
        }
      ],
      "source": [
        "most_keywords(df_league)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hvv6VB8O4Eb_"
      },
      "outputs": [],
      "source": [
        "df_top = clean_df('top_list.csv')\n",
        "most_keywords(df_top)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyApZhT54Eb_"
      },
      "outputs": [],
      "source": [
        "df_mid = clean_df('mid_list.csv')\n",
        "most_keywords(df_mid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KXWysxk4Eb_",
        "outputId": "32145cdb-4dc0-4352-eddc-efa3ebe6343f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "품사 태깅 10개만 출력 : [('와', 'Verb'), (',..', 'Punctuation'), ('씨발', 'Noun'), ('간', 'Suffix'), ('만에', 'Josa'), ('롤', 'Noun'), ('했더니', 'Verb'), ('요즘', 'Noun'), ('현자', 'Noun'), ('처럼', 'Josa')]\n",
            "명사 10개만 출력 : 씨발 롤 요즘 현자\n",
            "trigram 개수 : 853\n",
            "trigram 다섯개만 출력 : ['가기 프로젝트' '가기 프로젝트 구합' '가끔 정답' '가끔 정답 라인' '가미 유우']\n",
            "['상대 납골당 슈가', '챔피언 특정 챔피언', '랭크하닥 멘탈 정글', '픽해 듀오 챔피언', '진짜 개패 정글', '연구원 리듬게임', '대해 얘기 미드', '낭만 원딜', '뭔가 계속 차이', '보통 참여 미드', '간만 쌍욕 오늘', '카구야 서폿 느낌', '라면 만하 자꾸', '보통 상대 정글', '이재석 욕설 트롤', '일반 예전 렙때', '토리 미드 키사', '게임 챔프 스웨인', '예전 렙때 암살자', '기생 코바 원딜', '피지 가끔', '챔피언 이유 때문', '탈론 운영 달라', '미드 정글 구해', '언랭 언랭 여자', '참여 티어 언랭', '라면 만하', '정글 때문 메이지', '리쉬 보통 참여', '간만 쌍욕', '진짜 이해 처음', '정글 지금 채팅', '평생 행복 하자', '내전 참여 백구', '미드 낭만 원딜', '여자 인척 쌍벞주', '챔피언 정글 혼자', '저격 소리 참여', '바텀 유미 미드', '파우더 목표 카구야', '시발 대해 얘기', '라면 원영 몰입', '탱커 하싈분 웅애', '예능 제일', '라면 대학원생 아마', '진짜 진심 골드', '프로젝트 구합 승리', '라인 크립 보통', '꼽주 소리 내전', '덤덤 이빨 거시', '라인 눈앞 상대', '하나 몰루는데', '만하 랭크하닥 멘탈', '렙때 암살자 상대로', '유우 봇듀', '미드 가미 유우', '정글 만하 랭크하닥', '유미 유미 새끼', '문제 고요 게임', '승리 지향 즐겜', '봇듀 회장 원딜', '분만 만하 자꾸', '암살자 상대로 솔킬', '노틸러스 요즘 정글', '어딧음 심지어 가정', '미드 낭만', '지향 즐겜', '사람 예능', '진짜 소름 우리', '몰루는데 블리츠 바람', '하나 몰루는데 블리츠', '즐겜 모드', '샛기 어딧음 심지어', '관련 연구원 리듬게임', '욕설 트롤 그때', '목적 상대 캐치', '문제 재미', '도수 이재석 욕설', '처음 하든', '트롤 그때 챔피언', '캐치 묘수풀이 느낌', '즐겜 모드 조건', '피지 가끔 정답', '라면 혹시 트친', '욕설 트롤', '가미 유우 봇듀', '바람 문제 재미', '연구원 리듬게임 마이', '재미 진짜 소름', '문제 재미 진짜', '지향 즐겜 모드', '상대 캐치 묘수풀이', '느낌 피지 가끔', '사람 예능 제일', '정글 하나 몰루는데', '이해 처음 하든', '언랭 여자 인척', '예능 제일 스토리', '크립 보통 상대', '묘수풀이 느낌 피지']\n"
          ]
        }
      ],
      "source": [
        "df_jungle = clean_df('jungle_list.csv')\n",
        "most_keywords(df_jungle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QxQ50bA4EcA",
        "outputId": "24e4a749-7e5a-4a52-ff28-b0fafa73f541"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "품사 태깅 10개만 출력 : [('@bebeahrang', 'ScreenName'), ('발', 'Noun'), ('로란', 'Noun'), ('트', 'Noun'), ('같', 'Adjective'), ('이해', 'Noun'), ('요', 'Josa'), ('엄청', 'Adverb'), ('재미있는', 'Adjective'), ('게임', 'Noun')]\n",
            "명사 10개만 출력 : 발 로란 트 이해 \n",
            "trigram 개수 : 920\n",
            "trigram 다섯개만 출력 : ['가끔 미니' '가끔 미니 언도' '가끔 서폿' '가끔 서폿 원딜' '가끔 스왑']\n",
            "['환영 적막 게임', '하싈분 웅애 서폿', '팀운 하타 버러지', '게임 하싈분', '하이 텐션 환영', '딜로 유격대 마딜', '사람 친구 이상', '계약 다해', '가끔 실수', '잘해야 원딜 캐리', '유격대 마딜 그냥', '미드 원딜 정글', '캐리 이상 바텀터지', '원딜 중반 이후', '가끔 미니 언도', '라이너 명구 그냥', '미드 가미 유우', '사리 원딜 아기', '원딜 유격대', '보육 힐러 지오', '원딜 유격대 딜로', '원딜 제껍니', '기생 코바 원딜', '게임 터트려 아무', '아하 효월 원딜', '폿도 잘해야', '바텀터지 원딜 구원불', '비밀 물리 원딜', '유우 봇듀', '가면 서폿 그냥', '미드 가끔 서폿', '원딜 암살자 발견', '챔피언 정글 혼자', '버러지 아하 효월', '화남 연패', '제일 이해 원딜', '진짜 트롤 칼리', '바람 무관 대신', '진짜 박하', '로란 이해 게임', '진짜 진심 골드', '마법 미드 리리아', '유격대 딜로 유격대', '게임 카구야 학생회', '게임 터트렸는데', '터트렸는데 상체', '리쉬 보통 참여', '레드 가끔 실수', '화남 연패 동생', '마딜 그냥 비술사', '터트려 아무 원딜', '이해 혼자 친구', '주로 미드 가끔', '먼저 원딜 무도', '마법 미드 아하', '봇듀 회장 원딜', '원딜 주로 라인', '어쩌 어쩌 막상', '원딜 계약 다해', '제껍니 마이크 필수', '설치 사람 일단', '원딜 제껍니 마이크', '존나 화남 연패', '닉값 눈치 원딜', '혼자 친구 바람', '학살 이유 솔킬따', '진짜 트롤', '전사 먼저 원딜', '해괴 논리 진짜', '서폿 이해 혼자', '근딜거 평생', '가끔 서폿 원딜', '복귀 진짜 트롤', '계약 다해 진짜', '폿도 잘해야 원딜', '이상 엇으면어캄 게임', '그냥 비술사 보시', '가미 유우 봇듀', '원딜 게임 터트려', '친구 바람 혼자', '무관 대신 참고', '사람 게임 터트렸는데', '평생 웃기', '다해 진짜 박하', '친구 이상 엇으면어캄', '가끔 실수 막타침', '평생 웃기 그거슨', '게임 하싈분 웅애', '논리 진짜 트롤', '근딜 근딜거 평생', '정글 원딜 버러지', '엇으면어캄 게임 그냥', '막타침 이건 스킬', '게임 터트렸는데 상체', '웃기 그거슨 편견', '연습 원딜 제껍니', '게임 폿도 잘해야', '엇으면어캄 게임 카구야', '터트렸는데 상체 쌍련들', '근딜거 평생 웃기']\n"
          ]
        }
      ],
      "source": [
        "df_ad = clean_df('ad_list.csv')\n",
        "most_keywords(df_ad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6yl-dUS4EcA",
        "outputId": "e89870d6-1531-46fb-f15d-60fcfaffa2ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "품사 태깅 10개만 출력 : [('@bebeahrang', 'ScreenName'), ('발', 'Noun'), ('로란', 'Noun'), ('트', 'Noun'), ('같', 'Adjective'), ('이해', 'Noun'), ('요', 'Josa'), ('엄청', 'Adverb'), ('재미있는', 'Adjective'), ('게임', 'Noun')]\n",
            "명사 10개만 출력 : 발 로란 트 이해 \n",
            "trigram 개수 : 890\n",
            "trigram 다섯개만 출력 : ['가끔 미니' '가끔 미니 언도' '가끔 서폿' '가끔 서폿 원딜' '가끔 스왑']\n",
            "['사람 요즘 서폿', '모름 진짜 흡수', '진짜 빡쵸 실속', '관둔적잇엇음 진짜 지인', '대신 분노 주의', '사람 관둔적잇엇음 카구야', '캐리 이상 바텀터지', '서폿 전부 게임', '체감 공감 해도', '이태정 피방 상상', '크게 유미 유미', '주로 탱커 문도', '가끔 실수', '유미 유미 새끼', '진짜 트롤 칼리', '게임 사람 시작', '트윗 양해 바람', '사과 트윗', '주로 미드 가끔', '봇듀 회장 원딜', '마법 마법 미드', '개판 게임', '운영 타도 언제', '그냥 바보 눈물', '가끔 욱하', '트롤 서폿', '가끔 미니 언도', '어쩌 어쩌 막상', '전부 게임 사람', '미드 가미 유우', '뉴비 트윗', '벌레 시작 진짜', '리턴 거시 어푸푸', '초반 크게 유미', '점철 보이 채팅', '탱서폿 미드 마법', '지인 우리 혜지', '티어 서은광 혹시', '무관 대신 참고', '리쉬 보통 참여', '폿도 잘해야 원딜', '탱커 문도 갑자기', '설치 사람 일단', '가끔 욱하 아버', '가끔 서폿 원딜', '바람 트롤', '계약 다해 진짜', '적용 그냥 바보', '싹싹 그때 벌레', '다해 진짜 박하', '예정 오빠 픈데', '마법 미드 자식', '마법 미드 갑자기', '어푸푸 오늘 미드', '그때 벌레 시작', '특정 챔피언 정글', '이빨 거시 정글', '얼마 안대 서폿챔', '편지 진짜 빡쵸', '학생회 포지션 미코', '게임 해도', '사과 트윗 등등', '개판 게임 해도', '해도 온갖 혐오', '진짜 개판 게임', '눈물 가끔 욱하', '덤덤 이빨 거시', '어쩌 막상 게임', '유우 봇듀', '코딩 사천성', '트윗 양해', '최대한 딜넣음 오늘', '마법 미드 강우', '진짜 트롤', '트롤 칼리', '픈데 티어 서은광', '갑자기 쭈구러듬', '로란 이해 게임', '서폿 코딩 사천성', '갑자기 쭈구러듬 협곡', '미드 진짜 트롤', '이어도 얘기 혐오', '시작 이어도 얘기', '게임 는걸 원딜', '뉴비 트윗 양해', '말파 최대한 딜넣음', '양해 바람 트롤', '코딩 사천성 주변', '사람 시작 이어도', '막타침 이건 스킬', '가미 유우 봇듀', '제일 뉴비 트윗', '트롤 서폿 깜빡', '바보 눈물 가끔', '게임 해도 온갖', '게임 폿도 잘해야', '가끔 실수 막타침', '트롤 칼리 스타', '문도 갑자기 쭈구러듬', '유미 진짜 트롤']\n"
          ]
        }
      ],
      "source": [
        "df_sup = clean_df('sup_list.csv')\n",
        "most_keywords(df_sup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtotRU4V4EcA"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "f1d1ca04bcf6d8a22391b842d1bb739acd183735045253f19b5d1acca0a11b0e"
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('leagueoflegend')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "keybert.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}